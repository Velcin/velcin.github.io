{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<font size=6>**Rendu 1 : moteur de recherche**</font>\n",
    "\n",
    "Julien Velcin, Université Lyon 2 - Master Humanités Numériques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Le travail attendu :**\n",
    "\n",
    "- choisir un corpus assez long (au moins 1000 documents et 20k tokens en tout)\n",
    "- découpez le corpus en textes courts (par ex. des phrases ou des paragraphes)\n",
    "- prétraitez-le de différentes manières :\n",
    "    * TF et TFxIDF\n",
    "    * taille du vocabulaire (par ex. 500 mots les plus fréquents vs. 2000 mots)\n",
    "    * avec/sans les mots-outils\n",
    "- déployer un moteur de recherche sur votre corpus\n",
    "\n",
    "Il convient bien sûr d'interpréter cet énoncé avec ses propres mots, et peut-être faire des choix (ex. faire tel type de comparaison et pas tel autre). Il s'agit ici d'éléments de correction, donc je ne vais pas plus loin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui suit est un squelette de document donné à titre indicatif.\n",
    "\n",
    "Plan (indicatif) :\n",
    "\n",
    "- librairies utilisées, fonctions utiles\n",
    "- acquisition des données\n",
    "- construction des modèles\n",
    "- moteur de recherche\n",
    "- discussion\n",
    "- conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### librairies utilisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il faut adapter la liste à vos besoins\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.sparse import find, csr_matrix\n",
    "from scipy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# des options permettent de limiter (ou non) le nombre de lignes/colonnes affichées\n",
    "# par exemple :\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# fonction pour afficher une \"jolie\" représentation du vecteur v\n",
    "# ARGS :\n",
    "#   v : le vecteur à afficher (par ex. une ligne de la matrice X)\n",
    "#   features : le vocabulaire\n",
    "#   top_n : le nombre de mots maximum à afficher\n",
    "def print_feats(v, features, top_n = 30):\n",
    "    _, ids, values = find(v)\n",
    "    feats = [(ids[i], values[i], features[ids[i]]) for i in range(len(list(ids)))]\n",
    "    top_feats = sorted(feats, key=lambda x: x[1], reverse=True)[0:top_n]\n",
    "    return pd.DataFrame({\"word\" : [t[2] for t in top_feats], \"value\": [t[1] for t in top_feats]})   \n",
    "\n",
    "# affiche un concordancier pour le motif/mot *pat* dans la chaîne de caractères *texte*\n",
    "# *window* précise le nombre de caractères à afficher à droite et à gauche\n",
    "def concord(texte, pat, window = 50):\n",
    "    pattern = re.compile(pat)\n",
    "    res = pattern.finditer(texte)\n",
    "    pos_pattern = [m.span() for m in res]\n",
    "    context_left = pd.DataFrame({\"contexte gauche\": [texte[i-window:i-1] for (i, j) in pos_pattern]})\n",
    "    center = pd.DataFrame({\"passage\": [texte[i: j] for (i, j) in pos_pattern]})\n",
    "    context_right = pd.DataFrame({\"contexte droit\": [texte[j+1:j+window] for (i, j) in pos_pattern]})\n",
    "    return (pd.concat([context_left, center, context_right], axis=1))\n",
    "\n",
    "# fonction calculant le cosinus entre deux vecteurs\n",
    "def cosinus(i, j):\n",
    "        # numérateur : <i.j>\n",
    "    num = i.dot(j.transpose())[0,0]\n",
    "        # dénominateur : ||i||_2 * ||j||_2\n",
    "    den = norm(i.todense()) * norm(j.todense())\n",
    "    if (den>0): # on vérifie que le dénominateur n'est pas nul\n",
    "        return (num/den)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# retourne les *nb_top_docs* documents les plus similaires à la requête *query* dans la matrice *D*\n",
    "# *features* est associé au vocabulaire construit par la librairie\n",
    "def search(query, X, features, nb_top_docs=10):\n",
    "    indexes = [np.where(features == q)[0][0] for q in query if q in features]\n",
    "    query_vec = np.zeros(len(features))\n",
    "    query_vec[indexes] = 1\n",
    "    query_vec = csr_matrix(query_vec)\n",
    "    cc = {i: cosinus(X[i], query_vec) for i in range(n_docs)}\n",
    "    cc = sorted(cc.items(), key=lambda x: x[1], reverse=True)\n",
    "    return cc[0:nb_top_docs]    \n",
    "\n",
    "# affiche les *nb_top_docs* premiers documents\n",
    "def print_docs(result, nb_top_docs=10):\n",
    "    top_docs = [r for (r,v) in result[0:nb_top_docs]]\n",
    "    for i, td in zip(range(nb_top_docs), top_docs):\n",
    "        print(\"%s (%s): %s\" % (i+1, td, docs[td]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### acquisition des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(description, motivation et chargement du corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with open(votre fichier) as f:\n",
    "    lines = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "toute_la_chaine = \" \".join(lines).lower()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de lignes/documents : {len(lines)}\")\n",
    "\n",
    "une_chaine = \" \".join(lines)\n",
    "nb_tokens = len(une_chaine.split(\" \"))\n",
    "print(f\"Nombre de mots (token) : {nb_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taille du corpus et quelques exemples de textes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taille du corpus : {} documents\".format(len(lines)))\n",
    "print(\"Quelques exemples de textes situés au début du fichier :\")\n",
    "[l for l in lines[0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# la variable qui contient les documents\n",
    "docs = lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# le code suivant permet de regrouper les lignes par paragraphes (s'ils sont séparés par des espaces)\n",
    "\n",
    "docs = []\n",
    "s = \"\"\n",
    "for l in lines:\n",
    "    if (l != \"\"):\n",
    "        s = s + \" \" + l\n",
    "    else:\n",
    "        if (s != \"\"):\n",
    "            docs = docs + [s]\n",
    "            s = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nombre de documents : {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la fin, il faut que votre ayez constitué votre ensemble de documents dans la liste *docs*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construction de la matrice Documents x Termes pour différentes configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce qui suit est un exemple de configuration possible (TF...)\n",
    "\n",
    "tf_vectorizer = CountVectorizer()   # ici, on utilise l'encodage TF (mais d'autres sont possibles voire conseillés)\n",
    "tf_vectorizer.fit(lines)\n",
    "\n",
    "X = tf_vectorizer.transform(docs)\n",
    "\n",
    "features = tf_vectorizer.get_feature_names_out() # features permet de sauvegarder les noms des descriptions, càd les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_docs, n_terms = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# on fait la somme sur toutes les lignes pour chacun des mots\n",
    "tf_sum = X.sum(axis=0)\n",
    "tf_sum = tf_sum.tolist()[0] # conversion en liste\n",
    "\n",
    "print_feats(tf_sum, features, top_n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut utiliser le concordancier pour mieux comprendre le corpus (et peut-être enrichir la liste des mots outils)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concord(toute_la_chaine, \"mots / suite de mots à rechercher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans ce travail, on vous demander de tester différents paramètres pour le modèle :\n",
    "\n",
    "- pondération : TF, TFxIDF\n",
    "- mots-outil : avec ou sans stopwords\n",
    "- taille : nombre de mots dans le vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### moteur de recherche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie contient les différentes expérimentations que vous avez réalisé avec votre moteur de recherche.\n",
    "Il faut faire varier le modèle utilisée (sa configuration) mais également les requêtes.\n",
    "Pensez par exemple à des requêtes utilisant des mots très fréquents et d'autres utilisant des mots rares. Faites aussi varier la taille de la requête (nombre de mots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requete = # à compléter\n",
    "resultat = search(requete, X, features) # où X est la matrice des données et features la liste des mots du vocabulaire\n",
    "print_docs(resultat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie doit comporter une discussion sur les résultats obtenus dans la section précédente. Il faut essayer d'être synthétique en tirant quelques conclusions, discuter des points forts et des points faibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
